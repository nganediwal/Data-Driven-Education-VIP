# 11/22/2019

Final semester in VIP! It was a loaded one.

## Saturday, 11/16/2019

- Collaborated with Hosuk to successfully wrap multiple queries in distinct wrapped SQLAlchemy calls, produce distinct Pandas dataframes.
- Attempt to join these dataframes, discover that week calculation isn't working out correctly with TRUNC element in new query
- swap out SQL team's query for ```/sql/homework.sql``` in ```mergeWrapper.py``` (location of Pandas-based merging of SQL query results)
- Complete merging of null values in forum MongoDB query, incorporate it into ```mergeWrapper.py```. Fix its week calculation by saving a timestamp along with the week, obtaining course's starting timestamp from ```edx.courses```, and then subtracting to get absolute week.
- Merge results of 3 SQL queries and MongoDB query into single dataframe, with around 13,000 rows.
- Develop proof-of-concept aggregation step by iterating (inefficiently) over final dataframe, and at every row calculating an average of `load_video` for all entries with the same course, same student, and a week that is equal or prior to the current week. Add new `load_video_avg` column to the dataframe.

## Sunday, 11/17/2019

- Collaborate with team to expand aggregation to all features except week, course_id, user_id, and final_grade.
- Begin presentation with Ania, describe basic structure of presentation
- Abstract the specific course_id out to a command-line arg
- Abstract the PostgreSQL connect ID out to a command-line arg
- Drop all identifiable information from dataframe before writing to a CSV (encourage team to make use of Docker's temporary filesystem to prevent data leaks, even of this PII-stripped info).
- Final Dataframe can now be obtained

## Monday, 11/18/2019

- Retool ```models/run_models.py``` to accept data in new format, train 4 regression models
  - MLPRegressor
  - Gradient Boosting Regressors
  - Linear Regressor
  - Ridge Regressor
- Add in logic to either randomly shuffle data or select only data from Spring 2019 as test set, all other semesters as train set.
- Add in logic to select all features, only averaged features, or only non-averaged features (just this week) when training
- Perform Grid Search across models, hyperparameter tweaks, random seeds, and feature sets
  - Identify best hyperparameters for each of 4 models, by MSE (Mean Squared Error)
- Initiate training process, predicting final_grade on scale of 0 - 1

## Tuesday, 11/19/2019

- Training process terminates after about 8 hours
  - outputs results of 8160 models.
  - With best hyperparameters, retrain models and evaluate performance on test set, itemized by week
    - As expected, Mean Absolute Error and Mean Squared Error decrease on a curve over the semester, as we gain more data and produce better predictions.
    - Gradient Boosting obtains absolute error of about 8% on final grade by the end of the semester -- remarkable performance!
  - Work with Hosuk to visualize the graph of best models' performance over weeks of semester.
- Train model with Random Forest Regressor, identify best hyperparameters with grid search
  - Re-run with best hyperparameters, extract feature importance array from model after training
  - grade_avg is far and away most important feature, followed by load_video_avg
  - feature importance tapers off after top 6 features: grade_avg, load_video_avg, week, active_days_avg, problem_check_avg, link_clicked_avg
  - visualize feature importances with Plotly
- Perform entire grid search with all models & hyperparameters but new feature set: `top 6`, using these top 6 features only.
  - Once found best hyperparameters, graph performance vs week of test set once more.
  - Only small performance hit on MSE and MAE!!
- Add final dataframe, model training information, best hyperparameters, graphs to presentation
- Clean up report details

## Wednesday, 11/20/2019

- Present final presentation
  - Highlight feature selection - benefits include much faster train time, simpler models which may generalize better, and models which can be easily explained to students wondering where these predictions come from!

# To-Do:

- Come back next semester for presentations to see my team grow and improve!

# That's all for now, this is Manley Roberts signing off.
 
