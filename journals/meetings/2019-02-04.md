## 02/04/2019 - Subteam meeting

1. Checked in on everyone's progress in completing IRB certification for handling of sensitive FERPA-protected data, a prerequisite needed for data access. A few team members now have access to the first data source: Assignment data from EdX, from the C21U PostgreSQL relational database.
2. Found out timeline for other data sources. Canvas data is also hosted on a PostgreSQL database, but this one is controlled by a different department than C21U. This means we may have a slight delay on this data, but it might be relatively easy to manipulate once we do get access.
3. Found out that the clickstream data is hosted on a separate, expensive-to-operate MongoDB server. For this reason, we will be limiting access to this data to only a few people: Maxwell and Arjun.
4. Found out that forum data is relatively important, but Shawn might need to do a bit of investigation in order to find out how to obtain this data from Piazza.
5. Team has a discussion about the feature sets we're targeting. Primarily, feature sets from the Gardner & Brooks paper are also our target feature sets.
6. An exception! Maxwell brings up the possible feature of percentage of video material watched, across all videos or across videos for a particular section of the course relevant to a quiz. This brings up the possibility of crafting models suited to predict short-term success, although it means we would have more incompatible categories of models when comparing models with various techniques such as Bayesian Hierarchical Modeling. We decide that if we have the time, we may look into creating both long-term and short-term models.
7. Shawn suggests that the next meeting (2019-02-11) might be an excellent time for a deep dive into the Gardner & Brooks paper. The group agrees.
8. We split into sub-sub-teams. They are:
	* Clickstream: Maxwell, Arjun
	* Assignment: Manley, Yili, Okubay
9. We briefly talk over the purposes of the docker build environment: standardized tool sets, an environment which all scripts and tools should be able to run on, and ultimately, a deployable executable.
9. We briefly go through a docker tutorial, detailing the process of downloading the **ddesetup.sh** shell script, enabling it as executable if necessary, and then running it.
	* The script checks for updates on the dockerhub repo, pulls the most recent image, and then launches it into terminal.
	* The script turns the directory that it is in into a volume in the docker container, allowing for scripts in the github repo to be accessible from the container.
10. We discuss docker safety, including the fact that the docker repo is public. No data or sensitive code should ever make its way into the /docker/ directory in the github repo, as this may lead to it being packaged with **docker build** and hosted on the docker image.
