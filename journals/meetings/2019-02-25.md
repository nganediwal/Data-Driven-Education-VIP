## 02/25/2019 - General Meeting

Meeting tasks:
1. Discussing status of data - in terms of new data sources to make sense of EdX SQL Data. We have also planned a separate "crash course" in MongoDB for the MongoDB team.
2. Team reallocation - Manley shifted to MongoDB team.
3. Discussion of forum data. We decided that we needed to make the Piazza data a priority for use, as EdX forums seem relatively unused. Additional question: Gardner & Brooks claim that forum data was relatively useless by itself, while Crossley claims that forum data is very useful. We will do our best to include this data source and explore its usefulness.
4. Shawn presented core concepts of Gardner & Brooks paper and machine learning as a whole. https://learning-analytics.info/journals/index.php/JLA/article/view/5814
  - Machine learning: function approximation. Using a variety of models, seeking to optimize performance over a test set (future distribution) by training on a train set and developing a function. We are looking to minimize our error, which can be calculated by proportion of accurate classifications (matches to the correct value in a discrete set of outputs) or by minimizing squared error in regression problems. We mentioned that Null Hypothesis Statistical Testing can tell us whether a particular model's error, plotted as a distribution, is significantly smaller or larger than another's model.
  - There exist many ways to create valid models (decision trees, neural networks, boosting, support vector machines, kNN, naive Bayes, etc.). How do we evaluate which are better for particular problems (e.g. predicting student success in a course based on performance on subtasks in the course?)
  - Core concept: Gardner and Brooks' complexity is not about making better models, but figuring out how to better compare models. We can use naive average or null hypothesis statistical tests to evaluate accuracy, but this paper suggests a comparative measure (Bayesian Hierarchical Modeling) which is able to show whether one model is better than another, or whether the two are approximately equal in performance (ROPE). The diagram in the paper indicates the relative performance of a set of models, paired with data sets, and the gray square in the top left corner indicates a family of best models which are all approximately equal in performance (aka, several versions of adaboost and rpart, primarily).
